{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3167d7ef",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imread import imread\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "#from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from numpy import random as rng\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "train_folder = str(cwd) + '/images_background'\n",
    "val_folder = str(cwd) +  '/images_evaluation'\n",
    "save_path = str(cwd) +  '/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bfef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31958b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,c=loadimgs(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join(save_path,\"train.pickle\"), \"wb\") as f:\n",
    "#    pickle.dump((X,c),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval,yval,cval=loadimgs(val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eee2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join(save_path,\"val.pickle\"), \"wb\") as f:\n",
    "#    pickle.dump((Xval,cval),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ea258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cdfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),  # 64@96*96\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64@48*48\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.ReLU(),    # 128@42*42\n",
    "            nn.MaxPool2d(2),   # 128@21*21\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.ReLU(), # 128@18*18\n",
    "            nn.MaxPool2d(2), # 128@9*9\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.ReLU(),   # 256@6*6\n",
    "        )\n",
    "        self.liner = nn.Sequential(nn.Linear(9216, 4096), nn.Sigmoid())\n",
    "        self.out = nn.Linear(4096, 1)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.liner(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        out = self.out(dis)\n",
    "        #  return self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7890f6",
   "metadata": {},
   "source": [
    "Data Load function\n",
    "\n",
    "Returns: \n",
    "\n",
    "X: np.array of images\n",
    "\n",
    "y: np.array of labels, a number from 0 to 963\n",
    "\n",
    "lang_dict: dictionary of alphabets, each dictionary's entry is a range of values that belong to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    lang_dict = {}\n",
    "    classNo = 0\n",
    "    \n",
    "    for alphabet in sorted(os.listdir(dir_path)):\n",
    "        lang_dict[alphabet] = [classNo, None]\n",
    "        alpha_path = os.path.join(dir_path, alphabet)\n",
    "        \n",
    "        for character in sorted(os.listdir(alpha_path)):\n",
    "            cat_images = []\n",
    "            \n",
    "            for img in sorted(os.listdir(os.path.join(alpha_path, character))):\n",
    "                img_path = os.path.join(alpha_path, character, img)\n",
    "                cat_images.append(cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2GRAY))\n",
    "                y.append(classNo)\n",
    "                \n",
    "            classNo += 1\n",
    "            X.append(cat_images)\n",
    "        lang_dict[alphabet][1] = classNo-1\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776fbf1",
   "metadata": {},
   "source": [
    "Load train DS made up of 30 alphabets and the test DS made up of 20 alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d43b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, ld_train = load_data(train_folder)\n",
    "X_test, y_test, ld_test = load_data(val_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4c355",
   "metadata": {},
   "source": [
    "get_batch function\n",
    "\n",
    "input: batch_size and DS selector\n",
    "\n",
    "output:\n",
    "\n",
    "pairs: pairs of images that are non matching in the first half and matching in the second\n",
    "\n",
    "targets: np.array of 0s and 1s to sign if they're matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23865d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, ds = 'train', addi = 'norm'):\n",
    "    \n",
    "    #DS selector\n",
    "    if ds == 'train':\n",
    "        X = X_train\n",
    "    else:\n",
    "        X = X_test\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    cat = rng.choice(n_classes, size = batch_size, replace = False) # Sampling category without replacement\n",
    "    targets = np.zeros((batch_size,))\n",
    "    pairs = [np.zeros((batch_size, w, h, 1)) for _ in range(2)]        \n",
    "    if addi == 'norm':\n",
    "        targets[batch_size//2:] = 1\n",
    "\n",
    "        # \"pairs\" is a matrix of height 2 that contains examples to confront\n",
    "        # The first half has non matching examples, the second half has matching examples\n",
    "        for i in range(batch_size):\n",
    "            ex = rng.randint(n_examples) #example's number\n",
    "\n",
    "            #extracting the ex's example from i's category\n",
    "            #passing it to pairs' first example\n",
    "            pairs[0][i, :, :, :] = X[cat[i], ex, :, :].reshape(w, h, 1)\n",
    "            cat2 = 0\n",
    "            if i >= batch_size // 2:\n",
    "                cat2 = cat[i]\n",
    "            else:\n",
    "                cat2 = (cat[i] + rng.randint(1, n_classes)) % n_classes\n",
    "            ex2 = rng.randint(n_examples)\n",
    "\n",
    "            #same as earlier\n",
    "            pairs[1][i, :, :, :] = X[cat2, ex2, :, :].reshape(w, h, 1)\n",
    "    else:\n",
    "        print(\"Not norm\")\n",
    "        # Extract 1 matching pair and the rest are non-matching\n",
    "        extracted = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            ex = rng.randint(n_examples) #example's number\n",
    "\n",
    "            #extracting the ex's example from i's category\n",
    "            #passing it to pairs' first example\n",
    "            pairs[0][i, :, :, :] = X[cat[i], ex, :, :].reshape(w, h, 1)\n",
    "            \n",
    "            num = rng.randint(batch_size)\n",
    "            cat2 = 0\n",
    "            if num == 1 and extracted == 0:\n",
    "                extracted = 1\n",
    "                cat2 = (cat[i] + rng.randint(1, n_classes)) % n_classes \n",
    "            ex2 = rng.randint(n_examples)\n",
    "            pairs[1][i, :, :, :] = X[cat2, ex2, :, :].reshape(w, h, 1)\n",
    "\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5c6e7",
   "metadata": {},
   "source": [
    "one_shot function.\n",
    "\n",
    "Output: vector of pairs of characters, with only one matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(N, ds = 'val'):\n",
    "    if ds == 'train':\n",
    "        X = X_train\n",
    "    else:\n",
    "        X = X_test\n",
    "    \n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    cats = rng.choice(n_classes, size = (N,)) # sample N categories with repetition\n",
    "    indices = rng.choice(n_examples, size = (N,)) # sample N examples with repetition\n",
    "    cat = cats[0] # get first category\n",
    "    ex1 = rng.randint(n_examples) # sample example number\n",
    "    test_image = np.array([X[cat, ex1]] * N).reshape(N, w, h, 1) #sample a test image from first category\n",
    "    support_set = X[cats,indices].reshape(N,w,h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image, support_set, targets = shuffle(test_image, support_set, targets) #?\n",
    "    \n",
    "    return [test_image, support_set], targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d98ce",
   "metadata": {},
   "source": [
    "Tests accuracy of the one_shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_shot(model, N, k, ds = 'val'):\n",
    "    \n",
    "    # k is the number of examples we're testing\n",
    "    n_correct = 0\n",
    "    for l in range(k):\n",
    "        inputs, outputs = one_shot(N, ds)\n",
    "        \n",
    "        input_1 = torch.from_numpy(inputs[0]).to(device)\n",
    "        input_2 = torch.from_numpy(inputs[1]).to(device)\n",
    "        input1_test = torch.reshape(input_1,(N, 1, 105, 105)).to(device)\n",
    "        input2_test = torch.reshape(input_2,(N, 1, 105, 105)).to(device)\n",
    "        outputs = outputs.astype(int)\n",
    "        outputs = torch.from_numpy(outputs).to(device)\n",
    "\n",
    "        y_hat, output1, output2 = model(input1_test.float(), input2_test.float())\n",
    "        #loss = ContrastiveLoss()(output1, output2, outputs)\n",
    "    \n",
    "        \n",
    "        #preds = model(input1_test, input2_test) # change\n",
    "        #print (outputs.detach().numpy())\n",
    "        #print (y_hat.detach().numpy())\n",
    "        if np.argmax(outputs.cpu().detach().numpy()) == np.argmax(y_hat.cpu().detach().numpy()):\n",
    "        #if np.argmax(outputs) == np.argmax(y_hat): #convert y_hat to int\n",
    "        \t#print (np.argmax(outputs.cpu().detach().numpy()))\n",
    "        \t#print (np.argmax(y_hat.cpu().detach().numpy()))\n",
    "        \t#print (outputs)\n",
    "        \t#print (y_hat)\n",
    "        \tn_correct += 1\n",
    "        #print (n_correct)\n",
    "        #print (l)\n",
    "        #print(\"_____\")\n",
    "    return n_correct / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dec015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(batch_size, model, loss_fn, ds = 'train', addi = \"norm\"):\n",
    "    inputs , y = get_batch(batch_size, ds, addi) \n",
    "\n",
    "    input_1 = torch.from_numpy(inputs[0]).to(device)\n",
    "    input_2 = torch.from_numpy(inputs[1]).to(device)\n",
    "    input1_test = torch.reshape(input_1,(batch_size, 1, 105, 105)).to(device)\n",
    "    input2_test = torch.reshape(input_2,(batch_size, 1, 105, 105)).to(device)\n",
    "    y = y.astype(int)\n",
    "    y = torch.from_numpy(y).to(device)\n",
    "\n",
    "    y_hat, output1, output2 = model(input1_test.float(), input2_test.float())\n",
    "    #print (y_hat.size(), y.size())\n",
    "    y = torch.reshape(y,(batch_size, 1)).to(device)\n",
    "    loss = loss_fn(y_hat.float(), y.float()).to(device)\n",
    "    return y_hat, output1, output2, y, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86151c7a",
   "metadata": {},
   "source": [
    "SNN class.\n",
    "\n",
    "Contains: forward function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),  # 64@96*96\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64@48*48\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.ReLU(),    # 128@42*42\n",
    "            nn.MaxPool2d(2),   # 128@21*21\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.ReLU(), # 128@18*18\n",
    "            nn.MaxPool2d(2), # 128@9*9\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.ReLU(),   # 256@6*6\n",
    "        )\n",
    "        self.liner = nn.Sequential(nn.Linear(9216, 4096), nn.Sigmoid())\n",
    "        self.out = nn.Linear(4096, 1)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.liner(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        out = self.out(dis)\n",
    "        #  return self.sigmoid(out)\n",
    "        return out, out1, out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21faad50",
   "metadata": {},
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = torch.nn.Flatten() \n",
    "        self.layer1 = torch.nn.Linear(in_features=11025, out_features=1)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.flat(input1)\n",
    "        output1 = self.layer1(output1)\n",
    "        #out = torch.nn.functional.relu(out)\n",
    "        output2 = self.flat(input2)\n",
    "        output2 = self.layer1(output2)\n",
    "        return output1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b71ed",
   "metadata": {},
   "source": [
    "Loss function.\n",
    "\n",
    "Input: SNN\n",
    "\n",
    "Output: loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96be448",
   "metadata": {},
   "source": [
    "test_eval function.\n",
    "\n",
    "Used for testing and getting train/test accuracy.\n",
    "\n",
    "It receives batches of n pairs, with only one matching pair (to do function that creates it).\n",
    "\n",
    "If the output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beafd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, n, k, loss_fn, ds):\n",
    "\n",
    "    corr = 0\n",
    "    \n",
    "    for si in range(k):\n",
    "        y_hat, output1, output2,  y, loss = predict_model(n, model, loss_fn, ds, addi = \"test\")\n",
    "        \n",
    "        if np.argmax(y.cpu().detach().numpy()) == np.argmax(y_hat.cpu().detach().numpy()):\n",
    "            corr += 1\n",
    "    \n",
    "    return corr/k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loss_fn, optimizer):\n",
    "    loss_v = []\n",
    "    train_acc_v = []\n",
    "    val_acc_v = []\n",
    "    currTime = time.time()\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        y_hat, output1, output2,  y, loss = predict_model(batch_size, model, loss_fn)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print (f\"Iteration: {i}\")\n",
    "            print(y_hat, y)\n",
    "        '''\n",
    "        if i % evaluateEvery == 0:\n",
    "            loss_v.append(loss.item())\n",
    "            train_acc_v.append(test_eval(model, n, k, loss_fn, ds = 'train'))\n",
    "            val_acc_v.append(test_eval(model, n, k, loss_fn, ds = 'val'))   \n",
    "            \n",
    "            print(loss_v)\n",
    "            print(train_acc_v)\n",
    "            print(val_acc_v)\n",
    "            print(\"______\")\n",
    "        '''\n",
    "    return lossArr, trainAccArr, valAccArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 3000\n",
    "batch_size = 32\n",
    "evaluateEvery = 2000\n",
    "k = 10 #iterations\n",
    "n = 5 #batch size\n",
    "\n",
    "n_classes, n_examples, width, height = X_train.shape\n",
    "\n",
    "model = SNN().cuda()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(),lr = 0.0005 )\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 1e-4, alpha = 0.99, eps = 1e-8, weight_decay = 0.0005, momentum = 0.9)\n",
    "\n",
    "l_2, t_2, v_2 = train_epoch(model, nn.BCEWithLogitsLoss(),  optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b79af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
